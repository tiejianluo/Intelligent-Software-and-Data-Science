{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal  # Gaussian distribution utility class\n",
    "\n",
    "\n",
    "# -------------------------- Core Parameters and Data Generation --------------------------\n",
    "# True model parameters (target parameters to be estimated)\n",
    "true_mu = 3.0       # Mean of the true Gaussian distribution\n",
    "true_sigma = 1.5    # Standard deviation of the true Gaussian distribution\n",
    "\n",
    "def generate_data(n_samples=100):\n",
    "    \"\"\"\n",
    "    Generate observation data (sampling from the true distribution)\n",
    "    Input: n_samples - number of samples (default 100)\n",
    "    Output: torch.Tensor - sampled observation data\n",
    "    Significance: Simulates real-world observable data that implicitly contains information \n",
    "                 about the true parameters, serving as the basis for subsequent inference\n",
    "    \"\"\"\n",
    "    true_dist = Normal(true_mu, true_sigma)  # Define the true distribution\n",
    "    return true_dist.sample((n_samples,))    # Sample from the true distribution\n",
    "\n",
    "\n",
    "# -------------------------- Probability Model Definition --------------------------\n",
    "def prior_distribution():\n",
    "    \"\"\"\n",
    "    Define the prior distribution p(θ)\n",
    "    Output: Normal distribution instance - Gaussian prior with mean 0 and standard deviation 3\n",
    "    Significance: Initial belief about parameter θ without observation data \n",
    "                 (here we assume θ is more likely to be near 0)\n",
    "    \"\"\"\n",
    "    return Normal(loc=torch.tensor(0.0), scale=torch.tensor(3.0))\n",
    "\n",
    "def likelihood(x, theta):\n",
    "    \"\"\"\n",
    "    Define the likelihood function p(x|θ): probability of observation data x given parameter θ\n",
    "    Input:\n",
    "        x - observation data (torch.Tensor)\n",
    "        theta - candidate value of parameter θ (scalar)\n",
    "    Output: scalar - sum of log likelihoods (log p(x|θ))\n",
    "    Significance: Describes \"the likelihood of observing current data x under parameter θ\",\n",
    "                 serving as a bridge between data and parameters\n",
    "    \"\"\"\n",
    "    # Assume likelihood follows Gaussian distribution (mean=θ, fixed standard deviation=1)\n",
    "    # Return sum of log likelihoods for all data points\n",
    "    return Normal(loc=theta, scale=torch.tensor(1.0)).log_prob(x).sum()\n",
    "\n",
    "\n",
    "# -------------------------- Variational Posterior Definition --------------------------\n",
    "class VariationalPosterior:\n",
    "    \"\"\"\n",
    "    Approximate posterior distribution q(θ) = N(μ, σ²), using Gaussian distribution\n",
    "    to approximate the true posterior p(θ|x)\n",
    "    \n",
    "    The core idea is to optimize variational parameters μ and σ to make q(θ) \n",
    "    as close as possible to p(θ|x)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Variational parameters: mean μ and log standard deviation log_sigma\n",
    "        # Using log to ensure σ is positive after softplus transformation\n",
    "        self.mu = torch.tensor(0.0, requires_grad=True)  # Mean parameter (differentiable)\n",
    "        self.log_sigma = torch.tensor(0.0, requires_grad=True)  # Log standard deviation (differentiable)\n",
    "        \n",
    "    def distribution(self):\n",
    "        \"\"\"Return the currently parameterized Gaussian distribution\"\"\"\n",
    "        sigma = torch.nn.functional.softplus(self.log_sigma)  # Ensure standard deviation is positive\n",
    "        return Normal(loc=self.mu, scale=sigma)\n",
    "    \n",
    "    def sample(self, n_samples=1):\n",
    "        \"\"\"Sample from the approximate posterior (using rsample for reparameterization to enable gradient flow)\"\"\"\n",
    "        return self.distribution().rsample((n_samples,))\n",
    "\n",
    "\n",
    "# -------------------------- ELBO Calculation and Training --------------------------\n",
    "def elbo(x, q, prior, n_samples=10):\n",
    "    \"\"\"\n",
    "    Calculate the Evidence Lower Bound (ELBO): core objective function for variational inference\n",
    "    ELBO = E_q[log p(x|θ) + log p(θ) - log q(θ)]\n",
    "    \n",
    "    Input:\n",
    "        x - observation data\n",
    "        q - variational posterior instance\n",
    "        prior - prior distribution instance\n",
    "        n_samples - number of Monte Carlo samples (default 10)\n",
    "    Output: scalar - average ELBO value\n",
    "    Significance: ELBO is a lower bound on the log probability of the true posterior.\n",
    "                 Maximizing ELBO helps q(θ) approximate p(θ|x)\n",
    "    \"\"\"\n",
    "    thetas = q.sample(n_samples)  # Sample n_samples θ values from q(θ)\n",
    "    \n",
    "    elbo_values = []\n",
    "    for theta in thetas:\n",
    "        log_likelihood = likelihood(x, theta)  # Likelihood term: log p(x|θ)\n",
    "        log_prior = prior.log_prob(theta)       # Prior term: log p(θ)\n",
    "        log_q = q.distribution().log_prob(theta)  # Variational posterior term: log q(θ)\n",
    "        elbo_val = log_likelihood + log_prior - log_q  # ELBO for a single sample\n",
    "        elbo_values.append(elbo_val)\n",
    "    \n",
    "    return torch.mean(torch.stack(elbo_values))  # Average ELBO (reduces estimation variance)\n",
    "\n",
    "def train_vi(x, num_epochs=1000, lr=0.01, n_samples=10):\n",
    "    \"\"\"\n",
    "    Train the variational inference model: optimize variational parameters by maximizing ELBO\n",
    "    \n",
    "    Input:\n",
    "        x - observation data\n",
    "        num_epochs - number of training iterations (default 1000)\n",
    "        lr - learning rate (default 0.01)\n",
    "        n_samples - number of samples for ELBO calculation (default 10)\n",
    "    Output:\n",
    "        q - optimized variational posterior instance\n",
    "        elbo_history - record of ELBO changes during training (for plotting)\n",
    "    Significance: Gradient descent maximizes ELBO, gradually making q(θ) approximate the true posterior\n",
    "    \"\"\"\n",
    "    q = VariationalPosterior()  # Initialize variational posterior\n",
    "    optimizer = optim.Adam([q.mu, q.log_sigma], lr=lr)  # Optimizer (Adam)\n",
    "    elbo_history = []  # Record historical ELBO values\n",
    "    prior = prior_distribution()  # Prior distribution\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        current_elbo = elbo(x, q, prior, n_samples)  # Calculate current ELBO\n",
    "        loss = -current_elbo  # Maximizing ELBO is equivalent to minimizing negative ELBO\n",
    "        \n",
    "        # Gradient descent parameter update\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()        # Backpropagation to calculate gradients\n",
    "        optimizer.step()       # Update parameters\n",
    "        \n",
    "        elbo_history.append(current_elbo.item())  # Record ELBO\n",
    "        \n",
    "        # Print progress every 100 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            sigma = torch.nn.functional.softplus(q.log_sigma).item()\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, ELBO: {current_elbo.item():.2f}, \"\n",
    "                  f\"mu: {q.mu.item():.2f}, sigma: {sigma:.2f}\")\n",
    "    \n",
    "    return q, elbo_history\n",
    "\n",
    "\n",
    "# -------------------------- Main Function and Visualization --------------------------\n",
    "def main():\n",
    "    # Generate observation data (core input data)\n",
    "    x = generate_data(n_samples=100)\n",
    "    print(f\"Sample of generated data: {x[:5].numpy()}\")  # Print first 5 data points\n",
    "    \n",
    "    # Train VI model\n",
    "    q, elbo_history = train_vi(x, num_epochs=1000, lr=0.01)\n",
    "    \n",
    "    # Output final results (core output)\n",
    "    final_sigma = torch.nn.functional.softplus(q.log_sigma).item()\n",
    "    print(\"\\nFinal variational parameters:\")\n",
    "    print(f\"True mean: {true_mu}, Estimated mean: {q.mu.item():.2f}\")\n",
    "    print(f\"True standard deviation: {true_sigma}, Estimated standard deviation: {final_sigma:.2f}\")\n",
    "    \n",
    "    # Plot ELBO convergence curve (core visualization output)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(elbo_history)\n",
    "    plt.title('ELBO Convergence', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('ELBO Value', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot comparison between true distribution and approximate posterior (core visualization output)\n",
    "    thetas = np.linspace(-1, 7, 1000)\n",
    "    true_probs = np.exp([Normal(true_mu, true_sigma).log_prob(torch.tensor(t)).item() for t in thetas])\n",
    "    q_probs = np.exp([q.distribution().log_prob(torch.tensor(t)).item() for t in thetas])\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thetas, true_probs, label='True Posterior', color='blue')\n",
    "    plt.plot(thetas, q_probs, label='Variational Posterior', color='red', linestyle='--')\n",
    "    plt.axvline(x=true_mu, color='blue', linestyle=':', label=f'True Mean ({true_mu})')\n",
    "    plt.axvline(x=q.mu.item(), color='red', linestyle=':', label=f'Estimated Mean ({q.mu.item():.2f})')\n",
    "    plt.title('True vs Variational Posterior', fontsize=14)\n",
    "    plt.xlabel('θ', fontsize=12)\n",
    "    plt.ylabel('Probability Density', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
