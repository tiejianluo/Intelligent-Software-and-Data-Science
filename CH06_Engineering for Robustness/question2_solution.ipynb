{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fourth code segment: Implementing IRM + Bayesian Uncertainty on FastShip data\n",
    "class BayesianIRM(nn.Module):\n",
    "    \"\"\"\n",
    "    Bayesian IRM Model\n",
    "    \n",
    "    Input:\n",
    "        input_dim: input feature dimension\n",
    "        hidden_dim: hidden layer dimension\n",
    "        output_dim: output dimension\n",
    "    \n",
    "    Output:\n",
    "        Prediction results and uncertainty estimates\n",
    "    \n",
    "    Function:\n",
    "        Regression model combining causal invariance and Bayesian uncertainty\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=3, hidden_dim=32, output_dim=1):\n",
    "        super(BayesianIRM, self).__init__()\n",
    "        \n",
    "        # Feature extraction layer\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Prediction layer (mean)\n",
    "        self.predictor = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Uncertainty layer (log variance)\n",
    "        self.uncertainty = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Dropout for Bayesian approximation\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x, num_samples=5):\n",
    "        # Feature extraction\n",
    "        features = self.feature_extractor(x)\n",
    "        features = self.dropout(features)\n",
    "        \n",
    "        # Prediction\n",
    "        predictions = []\n",
    "        log_vars = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            # Apply dropout for Bayesian approximation\n",
    "            sampled_features = self.dropout(features)\n",
    "            pred = self.predictor(sampled_features)\n",
    "            log_var = self.uncertainty(sampled_features)\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            log_vars.append(log_var)\n",
    "        \n",
    "        # Average prediction and uncertainty\n",
    "        pred_mean = torch.mean(torch.stack(predictions), dim=0)\n",
    "        pred_var = torch.mean(torch.exp(torch.stack(log_vars)), dim=0)\n",
    "        \n",
    "        return pred_mean, pred_var\n",
    "\n",
    "def bayesian_irm_loss(pred_mean, pred_var, targets, env_id, penalty_weight=1e2):\n",
    "    \"\"\"\n",
    "    Bayesian IRM loss function\n",
    "    \n",
    "    Input:\n",
    "        pred_mean: predicted mean\n",
    "        pred_var: predicted variance\n",
    "        targets: true target values\n",
    "        env_id: environment ID\n",
    "        penalty_weight: penalty weight\n",
    "    \n",
    "    Output:\n",
    "        total_loss: total loss value\n",
    "    \n",
    "    Function:\n",
    "        Loss function combining Bayesian uncertainty and causal invariance\n",
    "    \"\"\"\n",
    "    # Bayesian negative log likelihood (Gaussian likelihood)\n",
    "    nll = 0.5 * torch.log(pred_var) + 0.5 * ((targets - pred_mean) ** 2) / pred_var\n",
    "    nll = nll.mean()\n",
    "    \n",
    "    # IRM penalty term (simplified implementation)\n",
    "    envs = torch.unique(env_id)\n",
    "    penalty = 0\n",
    "    \n",
    "    for e in envs:\n",
    "        env_mask = (env_id == e)\n",
    "        if torch.sum(env_mask) > 0:\n",
    "            env_pred_mean = pred_mean[env_mask]\n",
    "            env_targets = targets[env_mask]\n",
    "            env_loss = torch.mean(0.5 * ((env_targets - env_pred_mean) ** 2))\n",
    "            \n",
    "            # Calculate gradient penalty - need to create computation graph here\n",
    "            if env_pred_mean.requires_grad:\n",
    "                grad_params = torch.autograd.grad(\n",
    "                    env_loss, \n",
    "                    env_pred_mean, \n",
    "                    create_graph=True, \n",
    "                    retain_graph=True\n",
    "                )[0]\n",
    "                if grad_params is not None:\n",
    "                    penalty += torch.norm(grad_params) ** 2\n",
    "    \n",
    "    if len(envs) > 0:\n",
    "        penalty = penalty / len(envs)\n",
    "    else:\n",
    "        penalty = torch.tensor(0.0, device=pred_mean.device)\n",
    "    \n",
    "    total_loss = nll + penalty_weight * penalty\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def train_bayesian_irm_fastship(csv_file='fastship_data.csv'):\n",
    "    \"\"\"\n",
    "    Train Bayesian IRM model on FastShip dataset\n",
    "    \n",
    "    Input:\n",
    "        csv_file: dataset file path\n",
    "    \n",
    "    Output:\n",
    "        model: trained model\n",
    "        results: training and test results\n",
    "    \n",
    "    Function:\n",
    "        Load FastShip dataset and train order prediction model using Bayesian IRM algorithm\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    feature_cols = ['economic_index', 'trend', 'spurious_power']\n",
    "    X = df[feature_cols].values\n",
    "    y = df['orders'].values.astype(np.float32)\n",
    "    \n",
    "    # Environment ID\n",
    "    env_id = df['environment'].values.astype(np.int64)\n",
    "    \n",
    "    # Split data\n",
    "    train_mask = df['split'] == 'train'\n",
    "    test_mask = df['split'] == 'test'\n",
    "    \n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "    env_train, env_test = env_id[train_mask], env_id[test_mask]\n",
    "    \n",
    "    # Standardize features and targets\n",
    "    X_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "    \n",
    "    X_train = X_scaler.fit_transform(X_train)\n",
    "    X_test = X_scaler.transform(X_test)\n",
    "    \n",
    "    y_train = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test = y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    y_train = torch.FloatTensor(y_train)\n",
    "    env_train = torch.LongTensor(env_train)\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    y_test = torch.FloatTensor(y_test)\n",
    "    env_test = torch.LongTensor(env_test)\n",
    "    \n",
    "    # Define model\n",
    "    model = BayesianIRM(input_dim=3, hidden_dim=16, output_dim=1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Train model\n",
    "    epochs = 100\n",
    "    train_losses = []\n",
    "    test_mses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_mean, pred_var = model(X_train)\n",
    "        loss = bayesian_irm_loss(pred_mean.squeeze(), pred_var.squeeze(), y_train, env_train, penalty_weight=1e2)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_pred_mean, _ = model(X_test)\n",
    "            test_mse = torch.mean((test_pred_mean.squeeze() - y_test) ** 2)\n",
    "            test_mses.append(test_mse.item())\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item():.4f}, Test MSE: {test_mse.item():.4f}')\n",
    "    \n",
    "    print(f\"Final test MSE: {test_mses[-1]:.4f}\")\n",
    "    \n",
    "    # Analyze performance across different environments\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred_mean, test_pred_var = model(X_test)\n",
    "        test_pred = test_pred_mean.squeeze().numpy()\n",
    "        y_test_np = y_test.numpy()\n",
    "        env_test_np = env_test.numpy()\n",
    "        \n",
    "        # Weekday environment\n",
    "        weekday_mask_test = env_test_np == 0\n",
    "        if np.sum(weekday_mask_test) > 0:\n",
    "            weekday_mse = np.mean((test_pred[weekday_mask_test] - y_test_np[weekday_mask_test]) ** 2)\n",
    "        else:\n",
    "            weekday_mse = 0\n",
    "        \n",
    "        # Holiday environment (OoD)\n",
    "        holiday_mask_test = env_test_np == 1\n",
    "        if np.sum(holiday_mask_test) > 0:\n",
    "            holiday_mse = np.mean((test_pred[holiday_mask_test] - y_test_np[holiday_mask_test]) ** 2)\n",
    "        else:\n",
    "            holiday_mse = 0\n",
    "        \n",
    "        print(f\"Weekday test MSE: {weekday_mse:.4f}\")\n",
    "        print(f\"Holiday test MSE: {holiday_mse:.4f}\")\n",
    "        print(f\"OoD performance gap: {holiday_mse - weekday_mse:.4f}\")\n",
    "        \n",
    "        # Calculate uncertainty calibration\n",
    "        residuals = np.abs(test_pred - y_test_np)\n",
    "        uncertainties = np.sqrt(test_pred_var.squeeze().numpy())\n",
    "        uncertainty_correlation = np.corrcoef(residuals, uncertainties)[0, 1]\n",
    "        print(f\"Uncertainty-residual correlation: {uncertainty_correlation:.4f}\")\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses, \n",
    "        'test_mses': test_mses,\n",
    "        'weekday_mse': weekday_mse,\n",
    "        'holiday_mse': holiday_mse,\n",
    "        'uncertainty_correlation': uncertainty_correlation\n",
    "    }\n",
    "\n",
    "# Execute all code\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== Training Bayesian IRM model on FastShip data ===\")\n",
    "    bayesian_irm_model, bayesian_results = train_bayesian_irm_fastship()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
