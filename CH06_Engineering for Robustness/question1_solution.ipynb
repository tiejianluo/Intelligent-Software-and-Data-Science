{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def irm_loss(logits, labels, env_id, penalty_weight=1e2):\n",
    "    \"\"\"\n",
    "    Calculate IRM loss\n",
    "    \n",
    "    Input:\n",
    "        logits: model output logits\n",
    "        labels: true labels\n",
    "        env_id: environment ID\n",
    "        penalty_weight: penalty weight\n",
    "    \n",
    "    Output:\n",
    "        total_loss: total loss value\n",
    "    \n",
    "    Function:\n",
    "        Calculate invariant risk minimization loss, including empirical risk and invariance penalty\n",
    "    \"\"\"\n",
    "    # Empirical risk (cross entropy loss)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    empirical_risk = criterion(logits, labels)\n",
    "    \n",
    "    # Invariance penalty (simplified implementation)\n",
    "    # In actual IRM, this should calculate gradient penalty for environment-invariant features\n",
    "    envs = torch.unique(env_id)\n",
    "    penalty = 0\n",
    "    \n",
    "    for e in envs:\n",
    "        env_mask = (env_id == e)\n",
    "        if torch.sum(env_mask) > 0:\n",
    "            env_logits = logits[env_mask]\n",
    "            env_labels = labels[env_mask]\n",
    "            env_loss = criterion(env_logits, env_labels)\n",
    "            \n",
    "            # Calculate gradient penalty (simplified version)\n",
    "            grad_params = torch.autograd.grad(env_loss, logits, create_graph=True)[0]\n",
    "            penalty += torch.norm(grad_params) ** 2\n",
    "    \n",
    "    penalty = penalty / len(envs)\n",
    "    total_loss = empirical_risk + penalty_weight * penalty\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def train_irm_wilds(csv_file='wilds_reviews.csv'):\n",
    "    \"\"\"\n",
    "    Train IRM model on Wilds dataset\n",
    "    \n",
    "    Input:\n",
    "        csv_file: dataset file path\n",
    "    \n",
    "    Output:\n",
    "        model: trained model\n",
    "        results: training and test results\n",
    "    \n",
    "    Function:\n",
    "        Load Wilds dataset and train sentiment classification model using IRM algorithm\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    feature_cols = [f'text_feature_{i}' for i in range(10)]\n",
    "    X = df[feature_cols].values\n",
    "    y = df['rating'].values - 1  # Convert to 0-4 classes\n",
    "    \n",
    "    # Environment ID (reviewer ID)\n",
    "    env_id = df['reviewer_id'].values\n",
    "    \n",
    "    # Split data\n",
    "    train_mask = df['split'] == 'train'\n",
    "    test_mask = df['split'] == 'test'\n",
    "    \n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "    env_train, env_test = env_id[train_mask], env_id[test_mask]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    y_train = torch.LongTensor(y_train)\n",
    "    env_train = torch.LongTensor(env_train)\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    y_test = torch.LongTensor(y_test)\n",
    "    \n",
    "    # Define model\n",
    "    class SimpleClassifier(nn.Module):\n",
    "        def __init__(self, input_dim=10, hidden_dim=64, output_dim=5):\n",
    "            super(SimpleClassifier, self).__init__()\n",
    "            self.network = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, output_dim)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x)\n",
    "    \n",
    "    model = SimpleClassifier()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train model\n",
    "    epochs = 100\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(X_train)\n",
    "        loss = irm_loss(logits, y_train, env_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_logits = model(X_test)\n",
    "            test_preds = torch.argmax(test_logits, dim=1)\n",
    "            test_acc = accuracy_score(y_test.numpy(), test_preds.numpy())\n",
    "            test_accuracies.append(test_acc)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item():.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "    print(f\"Final test accuracy: {test_accuracies[-1]:.4f}\")\n",
    "    \n",
    "    return model, {'train_losses': train_losses, 'test_accuracies': test_accuracies}\n",
    "\n",
    "# Execute all code\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== Training IRM model on Wilds data ===\")\n",
    "    irm_model, irm_results = train_irm_wilds()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
