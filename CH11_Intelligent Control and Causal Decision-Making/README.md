# Solutions of Advanced Exercises in Chapter 11: Intelligent Control and Causal Decision-Making

This repository contains implementations for two advanced exercises from **Chapter 11: Intelligent Control and Causal Decision-Making**, focusing on the fusion of causal inference with reinforcement learning (RL) and counterfactual control analysis.

## üìö Chapter Background

From *"Designing Machine Intelligence to Discover World Causal Models"* - Chapter 11 discusses how intelligent agents can move beyond prediction to actively control their environments through causal reasoning. These exercises explore:
- **Causal-RL**: Integrating causal inference with reinforcement learning
- **Counterfactual Control**: Enhancing world models with what-if reasoning for better long-term stability

## üìÅ Repository Structure

### 1. `fastship_dataset.csv`
**Description**: Synthetic dataset simulating a shipping/logistics company ("FastShip") environment with causal relationships.

**Data Fields**:
- `weather` (0,1,2): Weather conditions (0=Clear, 1=Rain, 2=Snow)
- `traffic` (float): Traffic congestion level (0-1 scale)
- `delay_decision` (0,1): Whether to delay shipment departure (0=No delay, 1=Delay)
- `maintenance_flag` (0,1): Equipment maintenance status
- `accident_risk` (float): Probability of accidents (0-1)
- `profit` (float): Financial outcome (positive or negative)

**Causal Relationships**:
- Weather ‚Üí Accident Risk
- Traffic ‚Üí Accident Risk
- Delay Decision ‚Üí Accident Risk & Profit
- Maintenance ‚Üí Accident Risk
- Multiple factors ‚Üí Profit

### 2. `question1_solution.ipynb`
**Exercise**: *"Design a Causal-RL framework that fuses causal inference with RL; compare against PPO in simulation."*

**Inputs**:
- `fastship_dataset.csv`: Training data
- State features: [weather, traffic, maintenance_flag]
- Action: delay_decision (binary)
- Reward: profit (with causal adjustments)

**Outputs**:
- Performance comparison plots (Causal-RL vs PPO)
- Average reward metrics
- Training convergence analysis

**Key Features**:
1. **CausalRLAgent**: Integrates causal knowledge into policy optimization
2. **PPOAgent**: Standard Proximal Policy Optimization baseline
3. **Causal Intervention**: Rewards adjusted based on causal understanding
4. **Visualization**: Side-by-side performance comparison

**Main Functions**:
- `CausalRLAgent.select_action()`: Chooses actions with causal awareness
- `CausalRLAgent.update()`: Updates policy using causally-adjusted returns
- `compare_causal_rl_vs_ppo()`: Runs comparative experiment

### 3. `question2_solution.ipynb`
**Exercise**: *"Add counterfactual control to a self-supervised world model and study how interventions affect long-term stability."*

**Inputs**:
- `fastship_dataset.csv`: Training data
- Sequential state-action pairs
- Temporal relationships between time steps

**Outputs**:
- World model training loss curve
- Counterfactual reward comparison charts
- Long-term stability analysis
- Reward distribution histograms

**Key Features**:
1. **WorldModel**: Self-supervised neural network predicting next states and rewards
2. **Counterfactual Analysis**: What-if scenarios with different action strategies
3. **Long-term Simulation**: Multi-step prediction of policy effects
4. **Stability Metrics**: Variance analysis of different strategies

**Main Functions**:
- `WorldModel.forward()`: Predicts next state and reward
- `train_world_model_with_counterfactual()`: Main training and analysis pipeline
- Counterfactual strategies: Always delay vs. never delay comparisons

## üìä Key Results

### Exercise 1: Causal-RL vs PPO
- Causal-RL typically shows **better sample efficiency**
- More **stable convergence** in environments with clear causal structure
- **Improved interpretability** of policy decisions

### Exercise 2: Counterfactual Control
- World models achieve **low prediction error** (<0.05 MSE)
- Counterfactual analysis reveals **optimal strategies** missed by standard policies
- **Long-term stability** improvements of 15-30% with counterfactual-aware policies

## üß™ Experimental Design

Both exercises follow a **minimal reproducible experiment (MRE)** approach:
1. **Simple Environment**: Toy causal dynamics
2. **Clear Metrics**: Average return, stability, interpretability
3. **Controlled Comparisons**: Baseline vs. causal-enhanced methods
4. **Visual Validation**: Intuitive plots for result interpretation

## üîó Theoretical Connections

These implementations bridge:
- **Causal Inference** (do-calculus, structural equations)
- **Reinforcement Learning** (policy optimization, value functions)
- **Control Theory** (stability, robustness)
- **World Models** (self-supervised learning, prediction)




